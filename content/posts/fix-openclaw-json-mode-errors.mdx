# Fix OpenClaw JSON Mode Parsing Errors with DeepSeek R1

<div class="glass-card p-6 border-l-4 border-red-500 mb-8">

**Error Confirmation**

```bash
Error: JSON parsing failed
Expecting value: line 1 column 1 (char 0)
  at JSON.parse (<anonymous>)
  at OpenClawResponse.processResponse (/lib/processor.js:42)
```

**Full Response (The Problem):**

The model returns thinking tags before the JSON:

```text
Okay, the user wants a JSON response. I should format it properly...
```

```json
{"result": "actual data here"}
```

**Scope:** OpenClaw's JSON parser fails at position 0 because `<` is not an opening brace. DeepSeek R1 wraps responses in `think` tags before the actual JSON. OpenClaw receives the full response including thinking tags, and `JSON.parse()` fails.

**Error Code:** `Expecting value: line 1 column 1` — JSON parser encountered `<!-- think -->` instead of `{`.

</div>

<div class="glass-card p-6 border-l-4 border-blue-500 mb-8">

**Verified Environment**

| Component | Version | Last Verified |
|-----------|---------|---------------|
| OpenClaw | Latest stable | 2026-02-06 |
| DeepSeek R1 | 8b, 32b, 70b, distilled | 2026-02-06 |
| Ollama | Latest | 2026-02-06 |
| Operating System | Linux, macOS, WSL2 | 2026-02-06 |

**Note:** The thinking tags behavior is consistent across all DeepSeek R1 variants when JSON mode is expected.

</div>

---

## 3-Minute Sanity Check

Run these commands to confirm the issue:

```bash
# 1. Is DeepSeek R1 running?
ollama list | grep deepseek-r1
# Expected: deepseek-r1:latest (or variant)

# 2. Test raw JSON output (should show thinking tags)
ollama run deepseek-r1:latest 'Output JSON: {"status": "ok"}' | head -5
# Expected: You will see <!-- think --> or <think> tags before the JSON

# 3. Verify OpenClaw is configured for JSON mode
grep -i "json_mode\|json" ~/.openclaw/config.json 2>/dev/null || echo "Config file not found"
# Expected: json_mode setting or similar

# 4. Check if system prompt is already overridden
ollama run deepseek-r1:latest --system "You are a JSON API. Output ONLY JSON." 'Output JSON: {"status": "ok"}'
# Expected: Clean JSON output with NO thinking tags
```

**If step 2 shows NO thinking tags:** Your issue may not be DeepSeek thinking tags. Check for other JSON formatting issues.

**If step 4 works:** The system prompt override fixes the issue. Proceed to Primary Exit Path.

---

## Decision Gate

:::decision-gate
### Stop fighting VRAM physics.
### Should you keep debugging JSON parsing locally?

**Continue local debugging only if:**
- You can modify OpenClaw's system prompt configuration
- You have access to the Ollama/DeepSeek R1 configuration
- You have not already spent more than ~1 hour on this issue

**Stop here if any apply:**
- You already tried the system prompt override and JSON still fails
- OpenClaw crashes before sending the request (not a parsing issue)
- You are debugging JSON parsing for more than 1 hour
- Your OpenClaw version is outdated (>6 months old)

Past this point, debugging cost usually grows faster than results. If the system prompt override doesn't work, consider Secondary Exit Path.
:::

---

## Primary Exit Path: System Prompt Override

<div class="glass-card p-6 border-l-4 border-green-500 mb-8">

Override the default system prompt to explicitly disable thinking tags in DeepSeek R1.

**Why this works:**
- DeepSeek R1 respects system prompt instructions
- Disabling thinking tags at the source eliminates parsing issues
- No post-processing or code changes required
- Works across all DeepSeek R1 variants

**Time investment:** 5 minutes

**Steps:**

```bash
# Method 1: Command-line override (quickest)
ollama run deepseek-r1:latest \
  --system "You are a JSON-only API. Output valid JSON directly. Do NOT use <think> tags. Do NOT include any text before or after the JSON. Format: json object like {\"key\": \"value\"}" \
  --prompt 'Generate JSON with status ok'
```

```python
# Method 2: OpenClaw configuration file
# Add to ~/.openclaw/config.json or your project's openclaw_config.py

system_prompt = """You are a JSON-only API.
Rules:
1. Output ONLY valid JSON
2. No <think> tags
3. No markdown code blocks
4. No explanations before or after JSON

Format: json object like {"key": "value"}"""

# For Ollama integration
ollama_model = "deepseek-r1:latest"
ollama_system_prompt = system_prompt
```

**Verification:**

```bash
# Test the fix
ollama run deepseek-r1:latest \
  --system "You are a JSON-only API. Output ONLY JSON. No thinking tags." \
  'Output JSON: {"status": "ok"}'

# Expected output: {"status": "ok"}
# No <think> tags, no markdown, clean JSON
```

**For OpenClaw integration:**

```python
# openclaw_config.py
from openclaw import Client

client = Client(model="deepseek-r1:latest")

# Override system prompt to disable thinking tags
client.system_prompt = """You are a JSON-only API.
Rules:
1. Output ONLY valid JSON
2. No <think> tags
3. No markdown code blocks
4. No explanations before/after JSON

Format: json object like {"key": "value"}"""

# Now JSON responses will parse correctly
response = client.generate("Return json object with status ok")
print(response.parsed_json)  # Works!
```

</div>

---

## Secondary Exit Path (Conditional)

<div class="glass-card p-6 border-l-4 border-blue-500 mb-8">

**Use when:** Primary Exit Path fails (system prompt override doesn't work)

**Solution: Pre-processing to Strip Thinking Tags**

If you cannot modify the system prompt, strip the thinking tags before parsing:

```python
import re
import json

def extract_json_from_deepseek(response: str) -> str:
    """
    Extract JSON from DeepSeek R1 response,
    removing <think> tags if present.
    """
    # Remove thinking tags
    cleaned = re.sub(r'<think>.*?</think>', '', response, flags=re.DOTALL)

    # Extract JSON from markdown code blocks if present
    json_match = re.search(r'```json\s*(\{.*?\})\s*```', cleaned, flags=re.DOTALL)
    if json_match:
        return json_match.group(1)

    # Return cleaned response
    return cleaned.strip()

# Usage
raw_response = "<think>reasoning here</think>{\"result\": \"data\"}"
json_str = extract_json_from_deepseek(raw_response)
data = json.loads(json_str)
```

**When to use this:**
- You don't have control over system prompt configuration
- OpenClaw runs in a restricted environment
- You're integrating with an existing deployment that can't be reconfigured

**Time investment:** 10-15 minutes

**Note:** This is a workaround, not a fix. Use Primary Exit Path if possible.

</div>

---

## Why NOT Other Options

<div class="glass-card p-6 border-l-4 border-orange-500 mb-8">

| Option | Rejection Reason |
|--------|------------------|
| **Switch to a different model** | DeepSeek R1 works fine with proper configuration. Switching models avoids the root cause and may introduce new compatibility issues. |
| **Post-processing in OpenClaw source** | Requires modifying OpenClaw's core parsing logic. Changes break on updates and must be maintained. Fragile. |
| **Disable JSON mode entirely** | JSON mode is essential for structured output. Disabling it pushes parsing complexity into application code. |
| **Use regex in shell pipeline** | Fragile and error-prone. JSON structure varies; regex cannot reliably parse nested JSON. |
| **Report as OpenClaw bug** | Not a bug. OpenClaw correctly rejects invalid JSON. The issue is DeepSeek R1's thinking tags, which are outside the JSON spec. |
| **Wait for DeepSeek update** | Thinking tags are intentional architecture. No indication this will change. System prompt override works now. |

</div>

---

## Related Issues

<div class="glass-card p-6 border-l-4 border-orange-500 mb-8">

**Context Window Truncation**

Sometimes the JSON appears "cut off" mid-response:

```json
{"result": "partial data", "status":
```

**Root Cause:** DeepSeek R1 hit its context limit mid-generation.

**Fix:** Reduce context window usage:

```bash
ollama run deepseek-r1:latest \
  --num_ctx 8192 \
  --repeat-penalty 0.6
```

**For hardware limitations:** See [CUDA OOM Fix Guide](/guides/fix-openclaw-cuda-oom-errors).

</div>

---

**VRAM Warning:** If your VRAM is already near the limit, fixing JSON only delays the next OOM.

---

## Summary

| Check | Command | Pass Criteria |
|-------|---------|---------------|
| DeepSeek R1 installed | `ollama list \| grep deepseek-r1` | Shows deepseek-r1:latest or variant |
| Thinking tags present | `ollama run deepseek-r1:latest 'Output JSON' \| head -5` | Shows `<think>` or `<!-- think -->` tags |
| System prompt override works | `ollama run deepseek-r1:latest --system "JSON-only API" 'Output JSON'` | Clean JSON, no thinking tags |
| OpenClaw JSON mode enabled | Check config for `json_mode` setting | JSON mode is enabled |

**Decision:**

- **All pass:** Use Primary Exit Path (system prompt override). Takes 5 minutes.
- **System prompt override fails:** Use Secondary Exit Path (pre-processing). Takes 10-15 minutes.
- **Context truncation:** Reduce `--num_ctx` or upgrade hardware.

**Last resort:** If you have spent more than 1 hour on this, verify your DeepSeek R1 installation and consider using a model without thinking tags for JSON-only workflows.

---

## Related Guides

- [How to Use DeepSeek R1 with OpenClaw](/guides/how-to-use-deepseek-with-openclaw) - Complete DeepSeek R1 configuration guide
- [Fix OpenClaw CUDA OOM Errors](/guides/fix-openclaw-cuda-oom-errors) - VRAM optimization for DeepSeek R1
- [Hardware Requirements Reality Check](/guides/hardware-requirements-reality-check) - Can your hardware run DeepSeek R1?

---

## FAQ

<div itemscope itemType="https://schema.org/FAQPage">
  <div itemscope itemType="https://schema.org/Question" itemprop="mainEntity">
    <h3 itemprop="name">Q: Will this fix work with all DeepSeek R1 versions?</h3>
    <div itemscope itemType="https://schema.org/Answer" itemprop="acceptedAnswer">
      <p itemprop="text">
        <strong>A:</strong> Yes. The thinking tags behavior is consistent across DeepSeek R1 variants (8b, 32b, 70b, distilled versions). The system prompt override works for all of them.
      </p>
    </div>
  </div>

  <div itemscope itemType="https://schema.org/Question" itemprop="mainEntity">
    <h3 itemprop="name">Q: Do I need to reinstall DeepSeek R1?</h3>
    <div itemscope itemType="https://schema.org/Answer" itemprop="acceptedAnswer">
      <p itemprop="text">
        <strong>A:</strong> No. This is a configuration issue, not a model issue. The fix is applied at runtime through the system prompt. Your DeepSeek R1 installation is working as designed.
      </p>
    </div>
  </div>

  <div itemscope itemType="https://schema.org/Question" itemprop="mainEntity">
    <h3 itemprop="name">Q: Can I use this with other models like Llama 3?</h3>
    <div itemscope itemType="https://schema.org/Answer" itemprop="acceptedAnswer">
      <p itemprop="text">
        <strong>A:</strong> Yes. The system prompt override works with any model. However, the thinking tags issue is specific to DeepSeek R1. Other models (Llama, Mistral, etc.) don't use thinking tags by default.
      </p>
    </div>
  </div>
</div>

---

### Still Stuck? Check Your Hardware

Sometimes the code is fine, but the GPU is simply refusing to cooperate. Before you waste another hour debugging, compare your specs against the **[Hardware Reality Table](/guides/hardware-requirements-reality-check)** to see if you are fighting impossible physics.

---

<div class="text-center text-sm text-text-secondary mt-8 pt-4 border-t border-white/5">

☕ If this helped, you can [buy me a coffee](https://buymeacoffee.com/openclaw?ref=troubleshooting).

No obligation — just support for more real crash logs.

</div>

