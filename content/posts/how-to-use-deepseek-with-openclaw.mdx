---
title: "How to Run OpenClaw with DeepSeek R1: The Zero-Cost Local Agent Guide"
date: "2026-02-01"
description: "Claude API too expensive? Try the free DeepSeek R1. Step-by-step guide to configure DeepSeek with OpenClaw for zero-cost local AI agents."
tags: ["DeepSeek", "OpenClaw", "Tutorial", "Local Deployment", "Cost Optimization"]
category: "Tutorial"
featured: true
seoKeywords: ["DeepSeek R1", "OpenClaw tutorial", "Local LLM Agent", "DeepSeek API setup"]
---

> **üá®üá≥ ‰∏≠ÊñáÁî®Êà∑ËØ∑ÁÇπËøôÈáåÈòÖËØª‰∏≠ÊñáÁâàÊåáÂçó** [‰∏≠ÊñáÁâà](/blog/deepseek-guide-cn)

> **Disclaimer**: This guide is based on the latest DeepSeek and OpenClaw API documentation. Technology evolves quickly‚Äîif you encounter any setup issues, leave a comment below.

Claude API too expensive? OpenAI top-up a hassle? Give **DeepSeek R1** a try.

This guide walks you through setting up DeepSeek with OpenClaw‚Äîmaintaining **GPT-4 level reasoning** while reducing your API costs to **near zero**.

---

## Why DeepSeek R1 is Perfect for OpenClaw

OpenClaw with Claude Opus or GPT-4 performs exceptionally well, but run it for a month and your credit card statement might shock you.

Here's the key: **DeepSeek R1** delivers **GPT-4 tier performance** on Agent tasks and Chinese understanding‚Äîat **1/10th the cost** of Claude or less.

**Why choose DeepSeek R1:**

- ‚úÖ **Full OpenAI API compatibility**: Drop-in replacement for OpenClaw
- ‚úÖ **Chain of Thought (CoT)**: More stable on complex code tasks, fewer infinite loops
- ‚úÖ **New user bonus**: **5 million tokens** on signup (enough for months of OpenClaw)
- ‚úÖ **Local deployment**: Pair with Ollama and skip the API entirely

---

## Step 1: Get a DeepSeek API Key

1. **Visit the site**: [DeepSeek Platform](https://www.deepseek.com/)
2. **Sign up**: Phone or email registration available
3. **Create API Key**:
   - Click "API Keys" in the left sidebar
   - Click "Create API Key"
   - *Copy and save your key (starts with `sk-`)‚Äîit only displays once*

---

## Step 2: Configure OpenClaw (The Important Part)

We'll use **OpenAI compatibility mode** to integrate DeepSeek. Straightforward and reliable.

### Method A: Edit `.env` File (Recommended)

If you're running OpenClaw in a project, editing the `.env` file is the quickest approach.

Update your `.env` file:

```bash
# Set provider to OpenAI (DeepSeek is OpenAI-compatible)
LLM_PROVIDER="openai"

# ‚ö†Ô∏è CRITICAL: Must include /v1 suffix or requests will fail!
LLM_BASE_URL="https://api.deepseek.com/v1"

# Your DeepSeek API Key
LLM_API_KEY="sk-your-key-here"

# Use the reasoning model (R1)
LLM_MODEL="deepseek-reasoner"
# For cheaper V3 model, use "deepseek-chat"
```

### Method B: One-time Environment Variables (Linux/Mac)

Prefer not to edit config files? Run directly from terminal:

```bash
export LLM_PROVIDER="openai"
export LLM_BASE_URL="https://api.deepseek.com/v1"
export LLM_API_KEY="sk-your-key-here"
export LLM_MODEL="deepseek-reasoner"

# Then start OpenClaw
openclaw run "Analyze the codebase in this directory"
```

### Method C: Windows PowerShell

```powershell
$env:LLM_PROVIDER="openai"
$env:LLM_BASE_URL="https://api.deepseek.com/v1"
$env:LLM_API_KEY="sk-your-key-here"
$env:LLM_MODEL="deepseek-reasoner"
```

---

## Step 3: Verify Setup

Let's test your new AI agent's capabilities.

Run in terminal:

```bash
openclaw run "Explain the difference between DeepSeek R1 reasoning mode and normal mode"
```

**Watch the output**:

If you see the response plus thinking tags like ``

```, you've successfully enabled DeepSeek R1's reasoning engine.

---

> üí° **Pro Tip**: Can't remember CLI flags? Use our [OpenClaw Command Generator](/command-generator)‚Äîvisual builder, ready to copy-paste.

---

## Advanced: Run Locally (Completely Free)

Have a powerful machine (Mac M1/M2/M3 or NVIDIA GPU)? Run the model locally for fully offline, zero-cost operation.

### 1. Install Ollama

Download from [Ollama](https://ollama.com/).

### 2. Pull DeepSeek R1 Model

Run in terminal:

```bash
# Pull 7B version (for 8GB-16GB RAM)
ollama run deepseek-r1:7b

# Or 1.5B version (for older hardware)
ollama run deepseek-r1:1.5b
```

### 3. Point OpenClaw to Local Model

Edit your `.env`:

```bash
LLM_PROVIDER="openai"

# ‚ö†Ô∏è Note: Default Ollama port is 11434
LLM_BASE_URL="http://localhost:11434/v1"

LLM_API_KEY="ollama"  # Arbitrary value for local models

LLM_MODEL="deepseek-r1:7b"
```

**Note**: If running OpenClaw in Docker, use `host.docker.internal` instead of `localhost` or the container won't reach your host machine.

---

## Performance & Cost Comparison

| Method | Monthly Cost | Reasoning | Rating | Notes |
| :--- | :--- | :--- | :--- | :--- |
| **DeepSeek API** | $0 - $10 | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | (Recommended, free credits for new users) |
| **Local Ollama (7B)** | $0 | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | (Privacy-first, requires decent hardware) |
| **Claude Opus** | $100+ | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | (For those with unlimited budget) |

---

## FAQ

### Q: Why does OpenClaw throw "404 Not Found"?

**A**: 99% of the time you forgot the `/v1` suffix in your Base URL. Verify it's set to `https://api.deepseek.com/v1`.

### Q: What's the difference between `deepseek-reasoner` and `deepseek-chat`?

**A**: `reasoner` is R1 (reasoning model)‚Äîbest for coding, complex logic. `chat` is V3 (general model)‚Äîbetter for translation, copywriting. Faster and cheaper.

### Q: Do I need to modify OpenClaw source code?

**A**: Not at all. OpenAI compatibility mode handles everything.

### Q: Does function calling work?

**A**: Fully supported! DeepSeek is OpenAI-compatible, so all OpenClaw features work:

- ‚úÖ File read/write
- ‚úÖ Code execution
- ‚úÖ Tool calling
- ‚úÖ Multi-agent collaboration

---
