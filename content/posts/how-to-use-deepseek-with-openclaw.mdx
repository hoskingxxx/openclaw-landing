---
title: "Running OpenClaw with DeepSeek R1 (Local & API Guide)"
date: "2026-02-01"
description: "Technical guide for configuring DeepSeek R1 with OpenClaw. Covers API setup, local deployment via Ollama, and hardware requirements."
tags: ["DeepSeek", "OpenClaw", "Tutorial", "Local Deployment", "Configuration"]
category: "Tutorial"
featured: true
seoKeywords: ["DeepSeek R1", "OpenClaw configuration", "Local LLM setup", "DeepSeek API tutorial"]
---

> **中文用户请点这里阅读中文版指南** [中文版](/blog/deepseek-guide-cn)

## Overview

DeepSeek R1 is a reasoning-focused model that outputs explicit chain-of-thought traces before generating responses. This guide covers configuring OpenClaw to use R1 via API or local execution.

### Key Characteristics

- Explicit reasoning traces visible in output
- OpenAI API-compatible interface
- Lower API costs compared to proprietary models
- Available for local execution via Ollama

### Trade-offs

- **Slower than V3**: The reasoning step adds latency
- **Prompt format**: Requires specific formatting for optimal results
- **Hardware**: Local execution demands significant resources
- **Tool stability**: Function calling may vary compared to Claude

---

## Hardware Requirements

| Model Size | VRAM Required | RAM Required | Recommended Use Case |
| :--- | :--- | :--- | :--- |
| **1.5B** | N/A (CPU) | 8GB | Testing on older hardware |
| **7B** | 8GB | 16GB | General development tasks |
| **14B** | 16GB | 32GB | Complex reasoning tasks |
| **32B** | 24GB | 48GB | Production workloads |
| **70B** | 48GB | 96GB | Enterprise applications |

**Note**: Quantized models (Q4_K_M) reduce memory requirements by approximately 40%.

**API Mode**: No hardware requirement; introduces network latency.

---

## Method 1: API Configuration

### Step 1: Obtain API Key

1. Navigate to [DeepSeek Platform](https://www.deepseek.com/)
2. Register using phone or email
3. Access "API Keys" in the left sidebar
4. Generate a new key
5. Store the key securely—it is displayed only once

### Step 2: Configure Environment Variables

OpenClaw uses OpenAI-compatible endpoints. Edit your `.env` file:

```bash
# Provider configuration
LLM_PROVIDER="openai"

# Base URL must include /v1 suffix
LLM_BASE_URL="https://api.deepseek.com/v1"

# Your DeepSeek API key
LLM_API_KEY="sk-your-key-here"

# Model selection
LLM_MODEL="deepseek-reasoner"
# Alternative: "deepseek-chat" for V3 (faster, no reasoning)
```

> **Security Note**: Ensure `.env` is included in `.gitignore`. Do not commit API keys to version control.

### Step 3: Verify Configuration

```bash
openclaw run "Explain the difference between DeepSeek R1 reasoning mode and normal mode"
```

If configured correctly, the output will include thinking tags (``).

---

## Method 2: Local Deployment (Ollama)

Local execution provides zero API cost and offline operation.

### Step 1: Install Ollama

Download from [ollama.com](https://ollama.com/) for macOS, Linux, or Windows.

### Step 2: Pull Model

```bash
# 7B model (recommended for most systems)
ollama pull deepseek-r1:7b

# 1.5B model (for resource-constrained systems)
ollama pull deepseek-r1:1.5b
```

### Step 3: Configure OpenClaw

Edit `.env`:

```bash
LLM_PROVIDER="openai"
LLM_BASE_URL="http://localhost:11434/v1"
LLM_API_KEY="ollama"
LLM_MODEL="deepseek-r1:7b"
```

**Docker users**: Replace `localhost` with `host.docker.internal` to allow the container to reach the host.

---

## Model Comparison: R1 vs Claude 3.5 Sonnet

| Factor | DeepSeek R1 | Claude 3.5 Sonnet |
| :--- | :--- | :--- |
| **Reasoning** | Explicit CoT traces | Implicit reasoning |
| **Speed** | Slower (reasoning overhead) | Faster |
| **Tool Use** | Stable, OpenAI-compatible | More robust for complex tools |
| **Cost** | Lower ($0.55/M input tokens) | Higher ($3/M input tokens) |
| **Context** | 64K tokens | 200K tokens |
| **Local Option** | Yes (via Ollama) | No |

**Recommendation**: Use R1 for cost-sensitive workloads or when offline operation is required. Use Claude for complex tool-calling scenarios or when context length is critical.

---

## Troubleshooting

### 404 Not Found

The `LLM_BASE_URL` must include the `/v1` suffix:
```
https://api.deepseek.com/v1  # Correct
https://api.deepseek.com      # Incorrect
```

### Function Calling Issues

DeepSeek supports OpenAI-compatible function calling. If you experience issues:

1. Verify `LLM_PROVIDER="openai"` is set
2. Check that the model supports tools (`deepseek-reasoner` does)
3. Review OpenClaw logs for schema validation errors

### Performance Degradation

For local models:
- Ensure you are using an appropriately sized model for your hardware
- Consider using Q4_K_M quantization for lower VRAM usage
- Close unnecessary applications to free resources

For API mode:
- Check network latency to DeepSeek servers
- Consider switching to `deepseek-chat` (V3) for faster responses

---

## Additional Resources

- [OpenClaw Command Generator](/command-generator) — visual configuration builder
- [DeepSeek API Documentation](https://api-docs.deepseek.com/)
- [Ollama Model Library](https://ollama.com/library)
